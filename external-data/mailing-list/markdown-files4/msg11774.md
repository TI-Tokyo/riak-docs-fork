---
title: "Re: mapreduce timeout"
description: ""
project: community
lastmod: 2013-07-26T14:28:31-07:00
sitemap:
  priority: 0.2
layout: mailinglistitem
mailinglist_id: "msg11774"
mailinglist_parent_id: "msg11775"
author_name: "Deyan Dyankov"
project_section: "mailinglistitem"
sent_date: 2013-07-26T14:28:31-07:00
---


Hi Christian,

thank you for the detailed reply, It sheds light on some other issues that 
we're having and I'm beginning to believe that our map\_js\_vm\_count and 
reduce\_js\_vm\_count settings are set too low for our ring size.
I will definitely be learning some Erlang.

best regards,
Deyan

On Jul 23, 2013, at 9:32 AM, Christian Dahlqvist  wrote:

&gt; Hi Deyan,
&gt; 
&gt; As mentioned, it is recommended to write reduce phases it so that it can run 
&gt; recursively [1], so that you can avoid having to use the 
&gt; 'reduce\_phase\_only\_1' parameter. Once you have a reduce function that behaves 
&gt; this way, you can tune it by overriding the size of the 
&gt; 'reduce\_phase\_batch\_size' parameter, which by default is 20. You can also 
&gt; specify the 'do\_prereduce' parameter on the preceding map phase to make the 
&gt; first reduce iteration run in parallel across the cluster before finishing 
&gt; off at the coordinating node. This can significantly reduce the amount of 
&gt; data transferred across the cluster and speed up performance, although it may 
&gt; make the reduce phase functions a bit more difficult to write as they need to 
&gt; be able to handle a mix of output from the preceding map phase as well as 
&gt; results from previous iterations of the reduce phase.
&gt; 
&gt; As JavaScript map and reduce functions use VMs from a pool specified in the 
&gt; app.config file (map\_js\_vm\_count and reduce\_js\_vm\_count parameters), you will 
&gt; need to tune the size of these parameters based on your processing needs. As 
&gt; map phases run in parallel close to the partitions that hold the data, they 
&gt; often require considerably more VMs available than reduce phase functions. 
&gt; The exact number depends on the number of your ring size, the number of map 
&gt; phases you have in the job and the number of concurrent jobs you will be 
&gt; running. [2]
&gt; 
&gt; Using JavaScript functions is however considerably slower than using 
&gt; functions implemented in Erlang. For any functions that will execute 
&gt; regularly or as part of large jobs, we always recommend rewriting them in 
&gt; Erlang. In addition to speed things up, it removes the reliance on the 
&gt; JavaScript pools.
&gt; 
&gt; [1] http://docs.basho.com/riak/latest/dev/advanced/mapreduce/#How-Phases-Work
&gt; [2] 
&gt; http://riak-users.197444.n3.nabble.com/Follow-up-Riak-Map-Reduce-error-preflist-exhausted-td4024330.html
&gt; 
&gt; 
&gt; Best regards,
&gt; 
&gt; Christian
&gt; 
&gt; 
&gt; On 22 Jul 2013, at 17:03, Deyan Dyankov  wrote:
&gt; 
&gt;&gt; Thanks Christian,
&gt;&gt; 
&gt;&gt; I was able to modify the job code in a similar manner as you suggested and 
&gt;&gt; the issue is now resolved. However, I'd still like to understand the cause 
&gt;&gt; of these timeouts and what parameter should be raised, if possible, to 
&gt;&gt; mitigate them? This particular job was not expected to perform in real time 
&gt;&gt; and we were willing to wait for it. We may have other such cases in the 
&gt;&gt; future..
&gt;&gt; 
&gt;&gt; best regards,
&gt;&gt; Deyan
&gt;&gt; 
&gt;&gt; On Jul 15, 2013, at 4:49 PM, Christian Dahlqvist  wrote:
&gt;&gt; 
&gt;&gt;&gt; Hi Deyan,
&gt;&gt;&gt; 
&gt;&gt;&gt; When running mapreduce jobs, reduce phases often end up being the 
&gt;&gt;&gt; bottleneck. This is especially true when all input data needs to be 
&gt;&gt;&gt; gathered on the coordinating node before it can be executed, as is the case 
&gt;&gt;&gt; if the reduce\_phase\_only\_1 flag is enabled. Having this flag set will cause 
&gt;&gt;&gt; the mapreduce job to not scale very well.
&gt;&gt;&gt; 
&gt;&gt;&gt; Depending on your exact requirements, it may be worthwhile considering 
&gt;&gt;&gt; gathering the histogram data periodically, e.g. per hour and/or day. These 
&gt;&gt;&gt; aggregates can then be stored in separate buckets with a key that describes 
&gt;&gt;&gt; the content, e.g. \_\_ . Once this has been done, you can 
&gt;&gt;&gt; efficiently retrieve a limited number of objects that cover the period you 
&gt;&gt;&gt; want to get statistics for directly through the descriptive keys, and 
&gt;&gt;&gt; process these in the application layer. Even though this periodically 
&gt;&gt;&gt; requires a bit more work, it will most likely be much more efficient at 
&gt;&gt;&gt; query time and scale better.
&gt;&gt;&gt; 
&gt;&gt;&gt; Best regards,
&gt;&gt;&gt; 
&gt;&gt;&gt; Christian
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; On 14 Jul 2013, at 12:16, Deyan Dyankov  wrote:
&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Hi everyone,
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; first time here. Thanks in advance.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; I am experiencing issues with MapReduce and it seems to timeout after a 
&gt;&gt;&gt;&gt; certain volume data threshold is reached. The reducer is only one and here 
&gt;&gt;&gt;&gt; is the mapreduce initiation script:
&gt;&gt;&gt;&gt; #!/usr/bin/env ruby
&gt;&gt;&gt;&gt; […]
&gt;&gt;&gt;&gt; @client = Riak::Client.new(
&gt;&gt;&gt;&gt; :nodes =&gt; [
&gt;&gt;&gt;&gt; {:host =&gt; 'db1', :pb\_port =&gt; 8087, :http\_port =&gt; 8098},
&gt;&gt;&gt;&gt; {:host =&gt; 'db2', :pb\_port =&gt; 8087, :http\_port =&gt; 8098},
&gt;&gt;&gt;&gt; {:host =&gt; 'db3', :pb\_port =&gt; 8087, :http\_port =&gt; 8098}
&gt;&gt;&gt;&gt; ],
&gt;&gt;&gt;&gt; :protocol =&gt; 'pbc'
&gt;&gt;&gt;&gt; )
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; start\_key = "#{cust}:#{setup}:#{start\_time}"
&gt;&gt;&gt;&gt; end\_key = "#{cust}:#{setup}:#{end\_time}"
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; result = Riak::MapReduce.new(@client).
&gt;&gt;&gt;&gt; index(bucket\_name, index\_name, start\_key..end\_key).
&gt;&gt;&gt;&gt; map('map95th').
&gt;&gt;&gt;&gt; reduce('reduce95th', :arg =&gt; { 'reduce\_phase\_only\_1' =&gt; true }, :keep =&gt; 
&gt;&gt;&gt;&gt; true).
&gt;&gt;&gt;&gt; run()
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; puts result
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; The following is the code for the map95th and reduce95th javascript 
&gt;&gt;&gt;&gt; functions:
&gt;&gt;&gt;&gt; function map95th(v, keyData, arg) {
&gt;&gt;&gt;&gt; var key\_elements = v['key'].split(':');
&gt;&gt;&gt;&gt; var cust = key\_elements[0];
&gt;&gt;&gt;&gt; var setup = key\_elements[1];
&gt;&gt;&gt;&gt; var sid = key\_elements[2];
&gt;&gt;&gt;&gt; var ts = key\_elements[3];
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; var result\_key = cust + ':' + setup + ':' + ts;
&gt;&gt;&gt;&gt; var obj = {}
&gt;&gt;&gt;&gt; var obj\_data = Riak.mapValuesJson(v)[0];
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; obj\_data['bps'] = (obj\_data['rx\_bytes'] + obj\_data['tx\_bytes']) / 60;
&gt;&gt;&gt;&gt; return\_val = obj\_data['bps'];
&gt;&gt;&gt;&gt; return [ return\_val ];
&gt;&gt;&gt;&gt; }
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; // if used, this must be a single reducer! Call from Ruby like this:
&gt;&gt;&gt;&gt; // reduce('reduce95th', :arg =&gt; { 'reduce\_phase\_only\_1' =&gt; true }, :keep 
&gt;&gt;&gt;&gt; =&gt; true).
&gt;&gt;&gt;&gt; function reduce95th(values) {
&gt;&gt;&gt;&gt; var sorted = values.sort(function(a,b) { return a - b; });
&gt;&gt;&gt;&gt; var pct = sorted.length / 100;
&gt;&gt;&gt;&gt; var element\_95th = pct \* 95;
&gt;&gt;&gt;&gt; element\_95th = parseInt(element\_95th, 10) + 1;
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; return [ sorted[element\_95th] ];
&gt;&gt;&gt;&gt; }
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Now here is the interesting part. The MR goes through one record per 
&gt;&gt;&gt;&gt; minute. If I run it for a period of less than ~20 days, it executes. 
&gt;&gt;&gt;&gt; Otherwise, it times out:
&gt;&gt;&gt;&gt; [deyandyankov@azobook ~/repos/loshko/mapreduce/ruby (master)]$
&gt;&gt;&gt;&gt; [deyandyankov@azobook ~/repos/loshko/mapreduce/ruby (master)]$ ./95h.rb 
&gt;&gt;&gt;&gt; yellingtone default $((`date +%s` - 20 \* 86400)) `date +%s`
&gt;&gt;&gt;&gt; 125581.51666666666
&gt;&gt;&gt;&gt; [deyandyankov@azobook ~/repos/loshko/mapreduce/ruby (master)]$ ./95h.rb 
&gt;&gt;&gt;&gt; yellingtone default $((`date +%s` - 30 \* 86400)) `date +%s`
&gt;&gt;&gt;&gt; /Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client/beefcake\_protobuffs\_backend.rb:182:in
&gt;&gt;&gt;&gt; `decode\_response': Expected success from Riak but received 0. 
&gt;&gt;&gt;&gt; {"phase":1,"error":"timeout","input":null,"type":null,"stack":null} 
&gt;&gt;&gt;&gt; (Riak::ProtobuffsFailedRequest)
&gt;&gt;&gt;&gt; from 
&gt;&gt;&gt;&gt; /Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client/beefcake\_protobuffs\_backend.rb:116:in
&gt;&gt;&gt;&gt; `mapred'
&gt;&gt;&gt;&gt; from 
&gt;&gt;&gt;&gt; /Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client.rb:325:in
&gt;&gt;&gt;&gt; `block in mapred'
&gt;&gt;&gt;&gt; from 
&gt;&gt;&gt;&gt; /Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client.rb:435:in
&gt;&gt;&gt;&gt; `block in recover\_from'
&gt;&gt;&gt;&gt; from 
&gt;&gt;&gt;&gt; /Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/innertube-1.0.2/lib/innertube.rb:127:in
&gt;&gt;&gt;&gt; `take'
&gt;&gt;&gt;&gt; from 
&gt;&gt;&gt;&gt; /Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client.rb:433:in
&gt;&gt;&gt;&gt; `recover\_from'
&gt;&gt;&gt;&gt; from 
&gt;&gt;&gt;&gt; /Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client.rb:379:in
&gt;&gt;&gt;&gt; `protobuffs'
&gt;&gt;&gt;&gt; from 
&gt;&gt;&gt;&gt; /Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client.rb:133:in
&gt;&gt;&gt;&gt; `backend'
&gt;&gt;&gt;&gt; from 
&gt;&gt;&gt;&gt; /Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/client.rb:324:in
&gt;&gt;&gt;&gt; `mapred'
&gt;&gt;&gt;&gt; from 
&gt;&gt;&gt;&gt; /Users/deyandyankov/.rvm/gems/ruby-1.9.3-p392/gems/riak-client-1.1.1/lib/riak/map\_reduce.rb:217:in
&gt;&gt;&gt;&gt; `run'
&gt;&gt;&gt;&gt; from ./95h.rb:29:in `'
&gt;&gt;&gt;&gt; [deyandyankov@azobook ~/repos/loshko/mapreduce/ruby (master)]$
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; The records being processed look lie this:
&gt;&gt;&gt;&gt; {"rx\_bytes":3485395.0,"tx\_bytes":1658479.0}
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; When running the script with more than 20 days worth of data (two records 
&gt;&gt;&gt;&gt; per minute are processed, which amounts to 2 \* 60 \* 24 \* 20 = more than 
&gt;&gt;&gt;&gt; 57,600 processed), the script times out and here are some things from the 
&gt;&gt;&gt;&gt; logs:
&gt;&gt;&gt;&gt; ==&gt; /var/log/riak/erlang.log.1 &lt;==
&gt;&gt;&gt;&gt; Erlang has closed
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ==&gt; /var/log/riak/error.log &lt;==
&gt;&gt;&gt;&gt; 2013-07-14 13:03:51.580 [error] &lt;0.709.0&gt;@riak\_pipe\_vnode:new\_worker:768 
&gt;&gt;&gt;&gt; Pipe worker startup failed:fitting was gone before startup
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ==&gt; /var/log/riak/console.log &lt;==
&gt;&gt;&gt;&gt; 2013-07-14 13:03:51.584 [error] &lt;0.22049.4326&gt; gen\_fsm &lt;0.22049.4326&gt; in 
&gt;&gt;&gt;&gt; state wait\_for\_input terminated with reason: timeout
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ==&gt; /var/log/riak/error.log &lt;==
&gt;&gt;&gt;&gt; 2013-07-14 13:03:51.584 [error] &lt;0.22049.4326&gt; gen\_fsm &lt;0.22049.4326&gt; in 
&gt;&gt;&gt;&gt; state wait\_for\_input terminated with reason: timeout
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ==&gt; /var/log/riak/console.log &lt;==
&gt;&gt;&gt;&gt; 2013-07-14 13:03:51.940 [error] &lt;0.22049.4326&gt; CRASH REPORT Process 
&gt;&gt;&gt;&gt; &lt;0.22049.4326&gt; with 0 neighbours exited with reason: timeout in 
&gt;&gt;&gt;&gt; gen\_fsm:terminate/7 line 611
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ==&gt; /var/log/riak/crash.log &lt;==
&gt;&gt;&gt;&gt; 2013-07-14 13:03:51 =CRASH REPORT====
&gt;&gt;&gt;&gt; crasher:
&gt;&gt;&gt;&gt; initial call: riak\_pipe\_vnode\_worker:init/1
&gt;&gt;&gt;&gt; pid: &lt;0.22049.4326&gt;
&gt;&gt;&gt;&gt; registered\_name: []
&gt;&gt;&gt;&gt; exception exit: 
&gt;&gt;&gt;&gt; {timeout,[{gen\_fsm,terminate,7,[{file,"gen\_fsm.erl"},{line,611}]},{proc\_lib,init\_p\_do\_apply,3,[{file,"proc\_lib.erl"},{line,227}]}]}
&gt;&gt;&gt;&gt; ancestors: 
&gt;&gt;&gt;&gt; [&lt;0.710.0&gt;,&lt;0.709.0&gt;,riak\_core\_vnode\_sup,riak\_core\_sup,&lt;0.129.0&gt;]
&gt;&gt;&gt;&gt; messages: []
&gt;&gt;&gt;&gt; links: [&lt;0.710.0&gt;,&lt;0.709.0&gt;]
&gt;&gt;&gt;&gt; dictionary: 
&gt;&gt;&gt;&gt; [{eunit,[{module,riak\_pipe\_vnode\_worker},{partition,388211372416021087647853783690262677096107081728},{&lt;0.709.0&gt;,&lt;0.709.0&gt;},{details,{fitting\_details,{fitting,&lt;18125.23420.4566&gt;,#Ref&lt;18125.0.5432.50467&gt;,&lt;&lt;"C�������������������"&gt;&gt;,1},1,riak\_kv\_w\_reduce,{rct,#Fun,{struct,[{&lt;&lt;"reduce\_phase\_only\_1"&gt;&gt;,true}]}},{fitting,&lt;18125.23418.4566&gt;,#Ref&lt;18125.0.5432.50467&gt;,sink,undefined},[{log,sink},{trace,[error]},{sink,{fitting,&lt;18125.23418.4566&gt;,#Ref&lt;18125.0.5432.50467&gt;,sink,undefined}},{sink\_type,{fsm,10,infinity}}],64}}]}]
&gt;&gt;&gt;&gt; trap\_exit: false
&gt;&gt;&gt;&gt; status: running
&gt;&gt;&gt;&gt; heap\_size: 832040
&gt;&gt;&gt;&gt; stack\_size: 24
&gt;&gt;&gt;&gt; reductions: 1456611
&gt;&gt;&gt;&gt; neighbours:
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ==&gt; /var/log/riak/error.log &lt;==
&gt;&gt;&gt;&gt; 2013-07-14 13:03:51.940 [error] &lt;0.22049.4326&gt; CRASH REPORT Process 
&gt;&gt;&gt;&gt; &lt;0.22049.4326&gt; with 0 neighbours exited with reason: timeout in 
&gt;&gt;&gt;&gt; gen\_fsm:terminate/7 line 611
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ==&gt; /var/log/riak/crash.log &lt;==
&gt;&gt;&gt;&gt; 2013-07-14 13:03:52 =SUPERVISOR REPORT====
&gt;&gt;&gt;&gt; Supervisor: {&lt;0.710.0&gt;,riak\_pipe\_vnode\_worker\_sup}
&gt;&gt;&gt;&gt; Context: child\_terminated
&gt;&gt;&gt;&gt; Reason: timeout
&gt;&gt;&gt;&gt; Offender: 
&gt;&gt;&gt;&gt; [{pid,&lt;0.22049.4326&gt;},{name,undefined},{mfargs,{riak\_pipe\_vnode\_worker,start\_link,undefined}},{restart\_type,temporary},{shutdown,2000},{child\_type,worker}]
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ==&gt; /var/log/riak/console.log &lt;==
&gt;&gt;&gt;&gt; 2013-07-14 13:03:52.059 [error] &lt;0.710.0&gt; Supervisor 
&gt;&gt;&gt;&gt; riak\_pipe\_vnode\_worker\_sup had child undefined started with 
&gt;&gt;&gt;&gt; {riak\_pipe\_vnode\_worker,start\_link,undefined} at &lt;0.22049.4326&gt; exit with 
&gt;&gt;&gt;&gt; reason timeout in context child\_terminated
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; ==&gt; /var/log/riak/error.log &lt;==
&gt;&gt;&gt;&gt; 2013-07-14 13:03:52.059 [error] &lt;0.710.0&gt; Supervisor 
&gt;&gt;&gt;&gt; riak\_pipe\_vnode\_worker\_sup had child undefined started with 
&gt;&gt;&gt;&gt; {riak\_pipe\_vnode\_worker,start\_link,undefined} at &lt;0.22049.4326&gt; exit with 
&gt;&gt;&gt;&gt; reason timeout in context child\_terminated
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; The data is in leveldb and is accessed through secondary indexes. 
&gt;&gt;&gt;&gt; This is a 3 node cluster with 32GB ram, current usage is about 12G per 
&gt;&gt;&gt;&gt; node. n\_val=3. The same issues occurs on a similar 2 node cluster with 8GB 
&gt;&gt;&gt;&gt; of ram (usage is ~6G).
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; The following is my app.config:
&gt;&gt;&gt;&gt; [
&gt;&gt;&gt;&gt; {riak\_api, [
&gt;&gt;&gt;&gt; {pb\_ip, "0.0.0.0" },
&gt;&gt;&gt;&gt; {pb\_port, 8087 },
&gt;&gt;&gt;&gt; {pb\_backlog, 100 }
&gt;&gt;&gt;&gt; ]},
&gt;&gt;&gt;&gt; {riak\_core, [
&gt;&gt;&gt;&gt; {default\_bucket\_props, [
&gt;&gt;&gt;&gt; {n\_val, 3},
&gt;&gt;&gt;&gt; {last\_write\_wins, true}
&gt;&gt;&gt;&gt; ]},
&gt;&gt;&gt;&gt; {ring\_state\_dir, "/storage/riak/ring"},
&gt;&gt;&gt;&gt; {ring\_creation\_size, 256},
&gt;&gt;&gt;&gt; {http, [ {"0.0.0.0", 8098 } ]},
&gt;&gt;&gt;&gt; {https, [{ "0.0.0.0", 8069 }]},
&gt;&gt;&gt;&gt; {ssl, [
&gt;&gt;&gt;&gt; {certfile, "/etc/ssl/riak/server.crt"},
&gt;&gt;&gt;&gt; {cacertfile, "/etc/ssl/riak/root.crt"},
&gt;&gt;&gt;&gt; {keyfile, "/etc/ssl/riak/server.key"}
&gt;&gt;&gt;&gt; ]},
&gt;&gt;&gt;&gt; {handoff\_port, 8099 },
&gt;&gt;&gt;&gt; {dtrace\_support, false},
&gt;&gt;&gt;&gt; {enable\_health\_checks, true},
&gt;&gt;&gt;&gt; {platform\_bin\_dir, "/usr/sbin"},
&gt;&gt;&gt;&gt; {platform\_data\_dir, "/storage/riak"},
&gt;&gt;&gt;&gt; {platform\_etc\_dir, "/etc/riak"},
&gt;&gt;&gt;&gt; {platform\_lib\_dir, "/usr/lib/riak/lib"},
&gt;&gt;&gt;&gt; {platform\_log\_dir, "/var/log/riak"}
&gt;&gt;&gt;&gt; ]},
&gt;&gt;&gt;&gt; {riak\_kv, [
&gt;&gt;&gt;&gt; {storage\_backend, riak\_kv\_eleveldb\_backend},
&gt;&gt;&gt;&gt; {anti\_entropy, {on, []}},
&gt;&gt;&gt;&gt; {anti\_entropy\_build\_limit, {1, 3600000}},
&gt;&gt;&gt;&gt; {anti\_entropy\_expire, 604800000},
&gt;&gt;&gt;&gt; {anti\_entropy\_concurrency, 2},
&gt;&gt;&gt;&gt; {anti\_entropy\_tick, 15000},
&gt;&gt;&gt;&gt; {anti\_entropy\_data\_dir, "/storage/riak/anti\_entropy"},
&gt;&gt;&gt;&gt; {anti\_entropy\_leveldb\_opts, [{write\_buffer\_size, 4194304},
&gt;&gt;&gt;&gt; {max\_open\_files, 20}]},
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; {mapred\_name, "mapred"},
&gt;&gt;&gt;&gt; {mapred\_2i\_pipe, true},
&gt;&gt;&gt;&gt; {map\_js\_vm\_count, 16 },
&gt;&gt;&gt;&gt; {reduce\_js\_vm\_count, 12 },
&gt;&gt;&gt;&gt; {hook\_js\_vm\_count, 20 },
&gt;&gt;&gt;&gt; {js\_max\_vm\_mem, 8},
&gt;&gt;&gt;&gt; {js\_thread\_stack, 16},
&gt;&gt;&gt;&gt; {js\_source\_dir, "/etc/riak/mapreduce/js\_source"},
&gt;&gt;&gt;&gt; {http\_url\_encoding, on},
&gt;&gt;&gt;&gt; {vnode\_vclocks, true},
&gt;&gt;&gt;&gt; {listkeys\_backpressure, true},
&gt;&gt;&gt;&gt; {vnode\_mailbox\_limit, {1, 5000}}
&gt;&gt;&gt;&gt; ]},
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; {riak\_search, [
&gt;&gt;&gt;&gt; {enabled, true}
&gt;&gt;&gt;&gt; ]},
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; {merge\_index, [
&gt;&gt;&gt;&gt; {data\_root, "/storage/riak/merge\_index"},
&gt;&gt;&gt;&gt; {buffer\_rollover\_size, 1048576},
&gt;&gt;&gt;&gt; {max\_compact\_segments, 20}
&gt;&gt;&gt;&gt; ]},
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; {bitcask, [
&gt;&gt;&gt;&gt; {data\_root, "/storage/riak/bitcask"}
&gt;&gt;&gt;&gt; ]},
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; {eleveldb, [
&gt;&gt;&gt;&gt; {cache\_size, 1024},
&gt;&gt;&gt;&gt; {max\_open\_files, 64},
&gt;&gt;&gt;&gt; {data\_root, "/storage/riak/leveldb"}
&gt;&gt;&gt;&gt; ]},
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; {lager, [
&gt;&gt;&gt;&gt; {handlers, [
&gt;&gt;&gt;&gt; {lager\_file\_backend, [
&gt;&gt;&gt;&gt; {"/var/log/riak/error.log", error, 
&gt;&gt;&gt;&gt; 10485760, "$D0", 5},
&gt;&gt;&gt;&gt; {"/var/log/riak/console.log", info, 
&gt;&gt;&gt;&gt; 10485760, "$D0", 5}
&gt;&gt;&gt;&gt; ]}
&gt;&gt;&gt;&gt; ] },
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; {crash\_log, "/var/log/riak/crash.log"},
&gt;&gt;&gt;&gt; {crash\_log\_msg\_size, 65536},
&gt;&gt;&gt;&gt; {crash\_log\_size, 10485760},
&gt;&gt;&gt;&gt; {crash\_log\_date, "$D0"},
&gt;&gt;&gt;&gt; {crash\_log\_count, 5},
&gt;&gt;&gt;&gt; {error\_logger\_redirect, true}
&gt;&gt;&gt;&gt; ]},
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; {riak\_sysmon, [
&gt;&gt;&gt;&gt; {process\_limit, 30},
&gt;&gt;&gt;&gt; {port\_limit, 2},
&gt;&gt;&gt;&gt; {gc\_ms\_limit, 0},
&gt;&gt;&gt;&gt; {heap\_word\_limit, 40111000},
&gt;&gt;&gt;&gt; {busy\_port, true},
&gt;&gt;&gt;&gt; {busy\_dist\_port, true}
&gt;&gt;&gt;&gt; ]},
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; {sasl, [
&gt;&gt;&gt;&gt; {sasl\_error\_logger, false}
&gt;&gt;&gt;&gt; ]},
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; Sorry to bug you with such a long e-mail but I wanted to be as thorough as 
&gt;&gt;&gt;&gt; possible. I tried raising a few options but it didn't help: 
&gt;&gt;&gt;&gt; map\_js\_vm\_count, reduce\_js\_vm\_count, js\_max\_vm\_mem
&gt;&gt;&gt;&gt; I also tried adding a timeout argument to the map reduce caller code but 
&gt;&gt;&gt;&gt; even if I set it to 60,000 or more (this is milliseconds), the script is 
&gt;&gt;&gt;&gt; terminating with timeout error after 10-12 secs. The same behaviour is 
&gt;&gt;&gt;&gt; observed if I use http instead of pbc.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; What seems to be the problem? Is this a matter of configuration? I am 
&gt;&gt;&gt;&gt; surprised about the fact that the job runs with 20-25 days of data and not 
&gt;&gt;&gt;&gt; more.
&gt;&gt;&gt;&gt; 
&gt;&gt;&gt;&gt; thank you for your efforts,
&gt;&gt;&gt;&gt; Deyan
&gt;&gt;&gt;&gt; \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
&gt;&gt;&gt;&gt; riak-users mailing list
&gt;&gt;&gt;&gt; riak-users@lists.basho.com
&gt;&gt;&gt;&gt; http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com
&gt;&gt;&gt; 
&gt;&gt; 
&gt; 

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
riak-users mailing list
riak-users@lists.basho.com
http://lists.basho.com/mailman/listinfo/riak-users\_lists.basho.com

